name: Run unit tests

on:
  workflow_call:
    inputs:
      plugin:
        type: string
      os:
        type: string
      python-version:
        type: string

jobs:

  unit-tests:
    permissions:
      contents: read
    runs-on: ${{ inputs.os }}
    defaults:
      run:
        shell: bash
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Java for Windows
        if: inputs.os == 'windows-latest' && inputs.plugin == 'kedro-datasets'
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Set up Hadoop for Windows
        if: inputs.os == 'windows-latest' && inputs.plugin == 'kedro-datasets'
        shell: pwsh
        run: |
          # 1. Setup Hadoop Home
          $HADOOP_HOME = "C:\hadoop"
          $HADOOP_BIN = "$HADOOP_HOME\bin"
          New-Item -ItemType Directory -Force -Path $HADOOP_BIN

          # 2. Download Winutils (Hadoop 3.3.6)
          Invoke-WebRequest -Uri "https://github.com/cdarlint/winutils/raw/master/hadoop-3.3.6/bin/winutils.exe" -OutFile "$HADOOP_BIN\winutils.exe"
          Invoke-WebRequest -Uri "https://github.com/cdarlint/winutils/raw/master/hadoop-3.3.6/bin/hadoop.dll" -OutFile "$HADOOP_BIN\hadoop.dll"
          Copy-Item "$HADOOP_BIN\hadoop.dll" C:\Windows\System32\

          # 3. Create a Short Temp Directory (Crucial for Path Lengths)
          New-Item -ItemType Directory -Force -Path "C:\tmp\hive"

          # 4. Set Permissions using Winutils (Crucial for Mkdirs error)
          # We give 777 permission to C:\tmp so Spark can write freely
          & "$HADOOP_BIN\winutils.exe" chmod -R 777 C:\tmp

          # 5. Export Variables
          echo "HADOOP_HOME=$HADOOP_HOME" >> $env:GITHUB_ENV
          echo "$HADOOP_BIN" >> $env:GITHUB_PATH

          # Force python/spark to use this short path
          echo "TEMP=C:\tmp" >> $env:GITHUB_ENV
          echo "TMP=C:\tmp" >> $env:GITHUB_ENV

      - name: Install the latest version of uv with Python ${{inputs.python-version}}
        uses: astral-sh/setup-uv@v6
        with:
          version: "latest"
          python-version: ${{ inputs.python-version }}
          activate-environment: true

      - name: Install dependencies
        run: |
          uv pip install "kedro @ git+https://github.com/kedro-org/kedro@main"
          make plugin=${{ inputs.plugin }} install-test-requirements
          uv pip freeze

      - name: Run unit tests for kedro-airflow, kedro-docker, kedro-telemetry
        if: inputs.plugin != 'kedro-datasets'
        run: make plugin=${{ inputs.plugin }} test

      - name: Run unit tests for Linux / kedro-datasets
        if: inputs.os != 'windows-latest' && inputs.plugin == 'kedro-datasets'
        run: make dataset-tests

      # Todo: When SparkDatasetV2 replaces SparkDataset, change to: make dataset-tests
      - name: Run unit tests for Windows / kedro-datasets / no spark parallel
        if: inputs.os == 'windows-latest' && inputs.plugin == 'kedro-datasets'
        env:
          SPARK_LOCAL_IP: 127.0.0.1
          PYSPARK_PYTHON: python
          PYSPARK_DRIVER_PYTHON: python
        run: make test-no-spark

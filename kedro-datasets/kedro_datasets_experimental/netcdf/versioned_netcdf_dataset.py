"""VersionedNetCDFDataset loads and saves data to a local or remote netcdf (.nc) file with versioning support."""

from __future__ import annotations

import logging
from copy import deepcopy
from glob import glob
from pathlib import Path, PurePosixPath
from typing import Any

import fsspec
import xarray as xr
from kedro.io.core import (
    PROTOCOL_DELIMITER,
    AbstractVersionedDataset,
    DatasetError,
    Version,
    get_filepath_str,
    get_protocol_and_path,
)

logger = logging.getLogger(__name__)


class VersionedNetCDFDataset(AbstractVersionedDataset[xr.Dataset, xr.Dataset]):
    """``VersionedNetCDFDataset`` loads/saves data from/to a NetCDF file with versioning.
    It uses xarray to handle the NetCDF file.

    Example usage for the
    `YAML API <https://docs.kedro.org/en/stable/data/\
    data_catalog_yaml_examples.html>`_:

    .. code-block:: yaml

        single-file-versioned:
          type: netcdf.VersionedNetCDFDataset
          filepath: s3://bucket_name/path/to/folder/data.nc
          version: 2023-01-01T00.00.00.000Z
          save_args:
            mode: a
          load_args:
            decode_times: False

    Example usage for the
    `Python API <https://docs.kedro.org/en/stable/data/\
    advanced_data_catalog_usage.html>`_:

    .. code-block:: pycon

        >>> from kedro_datasets_experimental.netcdf import VersionedNetCDFDataset
        >>> import xarray as xr
        >>> from kedro.io import Version
        >>> ds = xr.DataArray(
        ...     [0, 1, 2], dims=["x"], coords={"x": [0, 1, 2]}, name="data"
        ... ).to_dataset()
        >>> dataset = VersionedNetCDFDataset(
        ...     filepath=tmp_path / "data.nc",
        ...     version=Version(None, "2023-01-01T00.00.00.000Z"),
        ...     save_args={"mode": "w"},
        ... )
        >>> dataset.save(ds)
        >>> reloaded = dataset.load()
        >>> assert ds.equals(reloaded)
    """

    DEFAULT_LOAD_ARGS: dict[str, Any] = {}
    DEFAULT_SAVE_ARGS: dict[str, Any] = {}
    DEFAULT_FS_ARGS: dict[str, Any] = {}

    def __init__(  # noqa
        self,
        *,
        filepath: str,
        temppath: str | None = None,
        load_args: dict[str, Any] | None = None,
        save_args: dict[str, Any] | None = None,
        version: Version | None = None,
        fs_args: dict[str, Any] | None = None,
        credentials: dict[str, Any] | None = None,
        metadata: dict[str, Any] | None = None,
    ):
        """Creates a new instance of ``VersionedNetCDFDataset`` pointing to a concrete NetCDF
        file on a specific filesystem with versioning support.

        Args:
            filepath: Filepath in POSIX format to a NetCDF file prefixed with a
                protocol like `s3://`. If prefix is not provided, `file` protocol
                (local filesystem) will be used. The prefix should be any protocol
                supported by ``fsspec``. It can also be a path to a glob. If a
                glob is provided then it can be used for reading multiple NetCDF
                files, but only when not using versioning (versioning is incompatible
                with globbed paths).
            temppath: Local temporary directory, used when reading from remote storage,
                since NetCDF files cannot be directly read from remote storage.
            load_args: Additional options for loading NetCDF file(s).
                Here you can find all available arguments when reading single file:
                https://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html
                Here you can find all available arguments when reading multiple files:
                https://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html
                All defaults are preserved.
            save_args: Additional saving options for saving NetCDF file(s).
                Here you can find all available arguments:
                https://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html
                All defaults are preserved.
            version: If specified, should be an instance of ``kedro.io.core.Version``.
                If its ``load`` attribute is None, the latest version will be loaded. If
                its ``save`` attribute is None, save version will be autogenerated.
            fs_args: Extra arguments to pass into underlying filesystem class
                constructor (e.g. `{"cache_regions": "us-east-1"}` for
                ``s3fs.S3FileSystem``).
            credentials: Credentials required to get access to the underlying filesystem.
                E.g. for ``GCSFileSystem`` it should look like `{"token": None}`.
            metadata: Any arbitrary metadata.
                This is ignored by Kedro, but may be consumed by users or external plugins.
        """
        _fs_args = deepcopy(fs_args) or {}
        _credentials = deepcopy(credentials) or {}

        protocol, path = get_protocol_and_path(filepath, version)

        # Check if path is a glob pattern (used for reading multiple NetCDF files)
        self._is_multifile = "*" in str(PurePosixPath(path).stem)

        # Versioning doesn't work with glob patterns
        if self._is_multifile and version:
            raise DatasetError(
                "Versioning is not supported for NetCDFDataset with glob patterns. "
                "Please use a single file path for versioning."
            )

        if protocol == "file":
            _fs_args.setdefault("auto_mkdir", True)
        elif protocol != "file" and temppath is None:
            raise ValueError(
                "Need to set temppath in catalog if NetCDF file exists on remote "
                + "filesystem"
            )

        if protocol == "https":
            raise DatasetError("Versioning is not supported for HTTP protocols.")

        self._protocol = protocol
        self._storage_options = {**_credentials, **_fs_args}
        self._fs = fsspec.filesystem(self._protocol, **self._storage_options)
        self._temppath = Path(temppath) if temppath is not None else None
        self.metadata = metadata

        # Handle default load and save arguments
        self._load_args = {**self.DEFAULT_LOAD_ARGS, **(load_args or {})}
        self._save_args = {**self.DEFAULT_SAVE_ARGS, **(save_args or {})}

        super().__init__(
            filepath=PurePosixPath(path),
            version=version,
            exists_function=self._exists,
            glob_function=self._fs.glob,
        )

    def _describe(self) -> dict[str, Any]:
        return dict(
            filepath=self._filepath,
            protocol=self._protocol,
            load_args=self._load_args,
            save_args=self._save_args,
            version=self._version,
        )

    def _get_versioned_path(self, version: str) -> PurePosixPath:
        """Create the versioned path for a given version.

        Args:
            version: The version to use for the path.

        Returns:
            The versioned path.
        """
        # Put the version in the parent directory, not in a subdirectory with the filename
        parent = self._filepath.parent
        stem = self._filepath.stem
        suffix = self._filepath.suffix
        versioned_path = parent / version / f"{stem}{suffix}"
        return versioned_path

    def _load(self) -> xr.Dataset:
        load_path = get_filepath_str(self._get_load_path(), self._protocol)
        multi_load_path = load_path

        # If NetCDF(s) are on any type of remote storage, need to sync to local to open.
        # Kerchunk could be implemented here in the future for direct remote reading.
        if self._protocol != "file":
            logger.info("Syncing remote NetCDF file to local storage.")

            if self._is_multifile:
                multi_load_path = sorted(self._fs.glob(load_path))  # type: ignore[assignment]

            self._fs.get(load_path, f"{self._temppath}/")
            load_path = f"{self._temppath}/{str(Path(load_path).stem)}.nc"

        if self._is_multifile and multi_load_path:
            data = xr.open_mfdataset(multi_load_path, **self._load_args)
        else:
            data = xr.open_dataset(load_path, **self._load_args)

        return data

    def _save(self, data: xr.Dataset) -> None:
        if self._is_multifile:
            raise DatasetError(
                "Globbed multifile datasets with '*' in filepath cannot be saved. "
                + "Create an alternate NetCDFDataset with a single .nc output file."
            )

        save_path = get_filepath_str(self._get_save_path(), self._protocol)

        # Fix the versioned path to ensure we don't try to create a directory with the same name as the file
        if self._version and self._version.save:
            # Check if the original file exists
            original_filepath = get_filepath_str(self._filepath, self._protocol)
            if self._fs.exists(original_filepath):
                raise DatasetError(
                    f"A file with the same name already exists in the directory "
                    f"{self._filepath.parent.as_posix()}"
                )

            # Create the parent directory structure if it doesn't exist
            parent_dir = str(PurePosixPath(save_path).parent)
            if self._protocol == "file" and not Path(parent_dir).exists():
                Path(parent_dir).mkdir(parents=True, exist_ok=True)

        if self._protocol == "file":
            data.to_netcdf(path=save_path, **self._save_args)
        else:
            if self._temppath is None:
                raise DatasetError("_temppath should have been set in __init__")
            temp_save_path = self._temppath / PurePosixPath(save_path).name
            data.to_netcdf(path=str(temp_save_path), **self._save_args)
            # Sync to remote storage
            self._fs.put_file(str(temp_save_path), save_path)

        self._invalidate_cache()

    def _exists(self, path=None) -> bool:
        """Check if the path exists.

        Args:
            path: Path to check for existence. If None, use the dataset's load path.

        Returns:
            Whether the path exists.
        """
        if path is None:
            try:
                path = get_filepath_str(self._get_load_path(), self._protocol)
            except DatasetError:
                return False

        if self._is_multifile:
            files = self._fs.glob(path)
            return bool(files)
        else:
            return self._fs.exists(path)

    def _release(self) -> None:
        """Release any resources held by the dataset."""
        super()._release()
        self._invalidate_cache()

    def _invalidate_cache(self) -> None:
        """Invalidate underlying filesystem caches."""
        filepath = get_filepath_str(self._filepath, self._protocol)
        self._fs.invalidate_cache(filepath)

    def __del__(self):
        """Cleanup temporary directory"""
        if self._temppath is not None:
            logger.info("Deleting local temporary files.")
            temp_filepath = self._temppath / PurePosixPath(self._filepath).stem
            if self._is_multifile:
                temp_files = glob(str(temp_filepath) + "*")
                for file in temp_files:
                    try:
                        Path(file).unlink()
                    except FileNotFoundError:  # pragma: no cover
                        pass  # pragma: no cover
            else:
                try:
                    file_to_remove = temp_filepath.with_suffix(".nc")
                    if file_to_remove.exists():
                        file_to_remove.unlink()
                except FileNotFoundError:
                    pass

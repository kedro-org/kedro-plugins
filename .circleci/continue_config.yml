version: 2.1

orbs:
  win: circleci/windows@2.4.1

# the default pipeline parameters, which will be updated according to
# the results of the path-filtering orb
parameters:
  run-build-kedro-telemetry:
    type: boolean
    default: false
  run-build-kedro-docker:
    type: boolean
    default: false
  run-build-kedro-airflow:
    type: boolean
    default: false
  run-build-kedro-datasets:
    type: boolean
    default: false
  release_package:
    type: string
    default: ""
  release_version:
    type: string
    default: ""

commands:
  setup_conda:
    parameters:
      python_version:
        type: string
    steps:
      - run:
          name: Cleanup pyenv
          command: sudo rm -rf .pyenv/ /opt/circleci/.pyenv/
      - run:
          name: Download and install miniconda
          command: |
            curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  > miniconda.sh
            bash miniconda.sh -b -p $HOME/miniconda
      - run:
          name: Create conda environment with correct python version
          command: |
            . /home/circleci/miniconda/etc/profile.d/conda.sh
            conda create --name kedro_plugins python=<<parameters.python_version>> -y
      - run:
          name: Setup bash env to run conda activation at each step
          command: |
            echo ". /home/circleci/miniconda/etc/profile.d/conda.sh" >> $BASH_ENV
            echo "conda deactivate; conda activate kedro_plugins" >> $BASH_ENV
            source $BASH_ENV

  setup_requirements:
    parameters:
      plugin:
        type: string
    steps:
      - run:
          name: Install pip setuptools
          command: make install-pip-setuptools
      - run:
          # pytables does not work properly with python 3.9 to handle our HDFDataSet
          # if pip-installed, so we install this dependency via conda
          name: Install pytables
          command: conda install -c conda-forge pytables -y
      - run:
          name: Install kedro and test requirements
          command: |
            cd <<parameters.plugin>>
            pip install git+https://github.com/kedro-org/kedro@main
            pip install -r test_requirements.txt
      - run:
          name: Install pre-commit hooks
          command: |
            cd <<parameters.plugin>>
            pre-commit install --install-hooks
            pre-commit install --hook-type pre-push
      - run:
          # this is needed to fix java cacerts so
          # spark can automatically download packages from mvn
          # https://stackoverflow.com/a/50103533/1684058
          name: Fix cacerts
          command: |
            sudo rm /etc/ssl/certs/java/cacerts
            sudo update-ca-certificates -f
      - run:
          # Since recently Spark installation for some reason does not have enough permissions to execute
          # /home/circleci/miniconda/envs/kedro_plugins/lib/python3.X/site-packages/pyspark/bin/spark-class.
          # So fixing it manually here.
          name: Fix Spark permissions
          command: sudo chmod -R u+x /home/circleci/miniconda/envs/kedro_plugins/lib/
      - run:
          name: Pip freeze
          command: pip freeze

  setup:
    parameters:
      python_version:
        type: string
      plugin:
        type: string
    steps:
      - checkout
      - setup_conda:
          python_version: <<parameters.python_version>>
      - setup_requirements:
          plugin: <<parameters.plugin>>

  # Windows specific commands
  win_setup_conda:
    # Miniconda3 is pre-installed on the machine:
    # https://circleci.com/docs/2.0/hello-world-windows
    parameters:
      python_version:
        type: string
    steps:
      - run:
          name: Initialize conda
          command: conda init powershell
      - run:
          name: Create 'kedro_plugins' conda environment
          command: conda create --name kedro_plugins python=<<parameters.python_version>> -y


  win_setup_env:
    steps:
      - run:
          # Required for Tensorflow tests
          name: Install Microsoft Visual C++ Redistributable
          command: |
            $ProgressPreference = "SilentlyContinue"
            Invoke-WebRequest https://aka.ms/vs/16/release/vc_redist.x64.exe -OutFile vc_redist.x64.exe
            .\vc_redist.x64.exe /S /v/qn
      - run:
          name: Install Java 8
          command: |
            $ProgressPreference = "SilentlyContinue"
            Invoke-WebRequest https://github.com/AdoptOpenJDK/openjdk8-upstream-binaries/releases/download/jdk8u252-b09/OpenJDK8U-jdk_x64_windows_8u252b09.zip -OutFile OpenJDK8U.zip
            Expand-Archive .\OpenJDK8U.zip -DestinationPath C:\OpenJDK8U
      - run:
          name: Create Inbound rules for Java
          command: |
            New-NetFirewallRule -DisplayName "Allow JDK UDP" -Profile "Public" -Protocol "UDP" -Direction Inbound -Program "C:\OpenJDK8U\openjdk-8u252-b09\bin\java.exe" -Action Allow
            New-NetFirewallRule -DisplayName "Allow JDK TCP" -Profile "Public" -Protocol "TCP" -Direction Inbound -Program "C:\OpenJDK8U\openjdk-8u252-b09\bin\java.exe" -Action Allow
      - run:
          name: Set Java environment variables
          command: |
            [Environment]::SetEnvironmentVariable("Path", [Environment]::GetEnvironmentVariable('Path', 'Machine') + ";C:\OpenJDK8U\openjdk-8u252-b09\bin", "Machine")
            setx /m JAVA_HOME "C:\OpenJDK8U\openjdk-8u252-b09"
      - run:
          name: Setup Hadoop binary
          command: |
            $ProgressPreference = "SilentlyContinue"
            Invoke-WebRequest https://github.com/steveloughran/winutils/raw/master/hadoop-2.6.3/bin/winutils.exe -OutFile winutils.exe
            New-Item -ItemType directory -Path C:\hadoop\bin
            mv .\winutils.exe C:\hadoop\bin
            setx /m HADOOP_HOME "C:\hadoop\"
      - run:
          name: Install 'make' command
          command: choco install make


  win_setup_requirements:
    parameters:
      plugin:
        type: string
      python_version:
        type: string
    steps:
      - run:
          name: Install GDAL, Fiona and pytables
          command: conda activate kedro_plugins; conda install gdal fiona pytables -c conda-forge -y
      - run:
          name: Install Kedro
          command: conda activate kedro_plugins; pip install git+https://github.com/kedro-org/kedro@main
      - run:
          name: Install all requirements
          command: conda activate kedro_plugins; cd <<parameters.plugin>>; pip install -r test_requirements.txt -U
      - run:
          name: Pip freeze
          command: conda activate kedro_plugins; pip freeze

jobs:
  unit_tests:
    parameters:
      python_version:
        type: string
      plugin:
        type: string
    machine:
      image: ubuntu-2004:202201-02
      docker_layer_caching: true
    steps:
      - setup:
          python_version: <<parameters.python_version>>
          plugin: <<parameters.plugin>>
      - run:
          name: Run unit tests
          command: make plugin=<<parameters.plugin>> test

  e2e_tests:
    parameters:
      python_version:
        type: string
      plugin:
        type: string
    machine:
      image: ubuntu-2004:202201-02
      docker_layer_caching: true
    steps:
      - setup:
          python_version: <<parameters.python_version>>
          plugin: <<parameters.plugin>>
      - run:
          name: Run e2e tests
          command: make plugin=<<parameters.plugin>> e2e-tests

  lint:
    parameters:
      plugin:
        type: string
    machine:
      image: ubuntu-2004:202201-02
      docker_layer_caching: true
    steps:
      - setup:
          python_version: "3.8"
          plugin: <<parameters.plugin>>
      - run:
          name: Run pylint and flake8
          command: make plugin=<<parameters.plugin>> lint

  win_unit_tests:
    parameters:
      python_version:
        type: string
      plugin:
        type: string
    executor:
      name: win/default
    steps:
      - checkout
      - win_setup_conda:
          python_version: <<parameters.python_version>>
      - win_setup_env
      - win_setup_requirements:
          plugin: <<parameters.plugin>>
          python_version: <<parameters.python_version>>
      # For anything not `kedro-datasets`
      - unless:
          condition:
            equal: ["kedro-datasets", <<parameters.plugin>>]

          # e2e tests are not currently runnable on CircleCI on Windows as
          # those require the ability to run Linux containers:
          # "The Windows executor currently only supports Windows containers.
          # Running Linux containers on Windows is not possible for now"
          # (from https://circleci.com/docs/2.0/hello-world-windows/)
          steps:
            - run:
                name: Run unit tests
                command: |
                  conda activate kedro_plugins
                  cd <<parameters.plugin>>
                  pytest tests

      - run:
          # geopandas and tensorflow conflicts when imported simultaneously.
          # The HDF5 header files used to compile this application do not match
          # the version used by the HDF5 library to which this application is linked.
          # Data corruption or segmentation faults may occur if the application continues.
          # This can happen when an application was compiled by one version of HDF5 but
          # linked with a different version of static or shared HDF5 library.
          # You should recompile the application or check your shared library related
          # settings such as 'LD_LIBRARY_PATH'.
          # You can, at your own risk, disable this warning by setting the environment
          # variable 'HDF5_DISABLE_VERSION_CHECK' to a value of '1'.
          # Setting it to 2 or higher will suppress the warning messages totally.
          name: Set HDF5_DISABLE_VERSION_CHECK environment variable
          command: setx /m HDF5_DISABLE_VERSION_CHECK 1
      - when:
          condition:
            and:
              - not:
                  equal: [ "3.10", <<parameters.python_version>> ]
              - equal: [ "kedro-datasets", <<parameters.plugin>> ]
          steps:
            - run:
                name: Run unit tests without spark in parallel
                command: conda activate kedro_plugins; make test-no-spark
      - when:
          condition:
            and:
              - equal: [ "3.10", <<parameters.python_version>> ]
              - equal: [ "kedro-datasets", <<parameters.plugin>> ]
          steps:
            - run:
                name: Run unit tests without spark sequentially
                command: conda activate kedro_plugins; make test-no-spark-sequential

  sync:
    parameters:
      python_version:
        type: string
    docker:
      # https://circleci.com/docs/2.0/circleci-images/#circleci-base-image
      - image: cimg/base:2020.01
    steps:
      - checkout
      - add_ssh_keys
      - run:
          name: Set git email and name
          command: |
            git config --global user.email "kedro@kedro.com"
            git config --global user.name "Kedro"
      # - run:
      #     name: Trigger Read The Docs build
      #     command: ./tools/circleci/rtd-build.sh ${RTD_TOKEN} latest
      - setup_conda:
          python_version: <<parameters.python_version>>
      - run:
          name: Maybe trigger the release workflow
          command: |
            conda activate kedro_plugins;
            pip install requests
            ./tools/circleci/circleci_release.py


  # This is effectively just a combination of the lint, unit_tests and e2e_tests jobs.
  # It's used to check that the nightly docker image is working ok and before publishing a release.
  build_package:
    parameters:
      python_version:
        type: string
    machine:
      image: ubuntu-2004:202201-02
      docker_layer_caching: true
    steps:
      - setup:
          python_version: <<parameters.python_version>>  # Just need one Python version here
          plugin: <<pipeline.parameters.release_package>>
      - run:
          name: Run linters
          command: export plugin=<<pipeline.parameters.release_package>>; make lint
      - unless:
          condition:
            equal: ["3.10", <<parameters.python_version>>]
          steps:
            - run:
                name: Run unit tests in parallel
                command: export plugin=<<pipeline.parameters.release_package>>; make test
      - when:
          condition:
            equal: [ "3.10", <<parameters.python_version>> ]
          steps:
            - run:
                name: Run unit tests sequentially
                command: export plugin=<<pipeline.parameters.release_package>>; make test-sequential
      - run:
          name: Run e2e tests
          command: make plugin=<<pipeline.parameters.release_package>> e2e-tests

  publish_package:
    machine:
      image: ubuntu-2004:202201-02
      docker_layer_caching: true
    steps:
      - run:
          name: Print the release package and version
          command: |
              echo "Release package: <<pipeline.parameters.release_package>> <<pipeline.parameters.release_version>>"
      - setup:
          python_version: "3.8"  # Just need one Python version here
          plugin: <<pipeline.parameters.release_package>>  # From circle_release.py
      - add_ssh_keys
      - run:
          name: Tag and publish release on Github
          command: ./tools/circleci/github_release.py <<pipeline.parameters.release_package>> <<pipeline.parameters.release_version>>
      - run:
          name: Publish to PyPI
          command: |
            export plugin=<<pipeline.parameters.release_package>>
            make package
            make pypi


workflows:
  # when pipeline parameter, run-build-kedro-telemetry is true, the
  # kedro-telemetry job is triggered.
  kedro-telemetry:
    when:
      and:
        - <<pipeline.parameters.run-build-kedro-telemetry>>
        - not: <<pipeline.parameters.release_package>>
        - not: <<pipeline.parameters.release_version>>
    jobs:
      - unit_tests:
          plugin: "kedro-telemetry"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - win_unit_tests:
          plugin: "kedro-telemetry"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - lint:
          plugin: "kedro-telemetry"
  # when pipeline parameter, run-build-kedro-docker is true, the
  # kedro-docker job is triggered.
  kedro-docker:
    when:
      and:
        -  <<pipeline.parameters.run-build-kedro-docker>>
        - not: <<pipeline.parameters.release_package>>
        - not: <<pipeline.parameters.release_version>>
    jobs:
      - unit_tests:
          plugin: "kedro-docker"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - e2e_tests:
          plugin: "kedro-docker"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - win_unit_tests:
          plugin: "kedro-docker"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - lint:
          plugin: "kedro-docker"
  # when pipeline parameter, run-build-kedro-airflow is true, the
  # kedro-airflow job is triggered.
  kedro-airflow:
    when:
      and:
        - <<pipeline.parameters.run-build-kedro-airflow>>
        - not: <<pipeline.parameters.release_package>>
        - not: <<pipeline.parameters.release_version>>
    jobs:
      - unit_tests:
          plugin: "kedro-airflow"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
          pre-steps:
          - run:
              name: Avoid GPL dependency (unidecode)
              command: echo 'export SLUGIFY_USES_TEXT_UNIDECODE=yes' >> $BASH_ENV
      - e2e_tests:
          plugin: "kedro-airflow"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - win_unit_tests:
          plugin: "kedro-airflow"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - lint:
          plugin: "kedro-airflow"
  # when pipeline parameter, run-build-kedro-datasets is true, the
  # kedro-datasets job is triggered.
  kedro-datasets:
    when:
      and:
        - <<pipeline.parameters.run-build-kedro-datasets>>
        - not: <<pipeline.parameters.release_package>>
        - not: <<pipeline.parameters.release_version>>
    jobs:
      - unit_tests:
          plugin: "kedro-datasets"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - win_unit_tests:
          plugin: "kedro-datasets"
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - lint:
          plugin: "kedro-datasets"

  # For release
  main_updated:
    when:
      and:
        - not: <<pipeline.parameters.release_package>>
        - not: <<pipeline.parameters.release_version>>
    jobs:
      - sync:
          filters:
            branches:
              only: main
          matrix:
            # We just need one Python enviornment to trigger the job
            parameters:
              python_version: ["3.8"]

  package_release:
    when:
      and:
        -  <<pipeline.parameters.release_package>>
        -  <<pipeline.parameters.release_version>>
    jobs:
      - build_package:
          matrix:
            parameters:
              python_version: ["3.7", "3.8", "3.9", "3.10"]
      - publish_package:
          requires:
            - build_package
